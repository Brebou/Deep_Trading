{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf4f1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/user/miniconda3/lib/python3.13/site-packages (3.10.6)\n",
      "Requirement already satisfied: scikit-learn in /home/user/miniconda3/lib/python3.13/site-packages (1.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/user/miniconda3/lib/python3.13/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/user/miniconda3/lib/python3.13/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/user/miniconda3/lib/python3.13/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/user/miniconda3/lib/python3.13/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/user/miniconda3/lib/python3.13/site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/user/miniconda3/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/user/miniconda3/lib/python3.13/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/user/miniconda3/lib/python3.13/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/user/miniconda3/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /home/user/miniconda3/lib/python3.13/site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /home/user/miniconda3/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /home/user/miniconda3/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/user/miniconda3/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Device : cpu\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "%pip install matplotlib scikit-learn\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time, strftime, gmtime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import Model_Blocks.Transformer as Transformer \n",
    "from train_model import *\n",
    "from trading import *\n",
    "from data_prep import *\n",
    "\n",
    "# Changing to mps if available\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device :', device)\n",
    "\n",
    "dataset_path = 'data/stocks.csv'\n",
    "model_path = ''\n",
    "lr = 3e-4\n",
    "batch_size = 8\n",
    "save_dir = 'outputs'\n",
    "nepochs = 100\n",
    "num_layers = 4\n",
    "hidden_dim = 512\n",
    "n_stocks = 26\n",
    "context_size = 256\n",
    "label_smoothing = 0.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ce353c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path : data/stocks.csv\n",
      "n_stocks : 26\n",
      "batch_size : 8\n",
      "Growth size : (3163, 26)\n",
      "Labels : [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] of size (3163, 26)\n",
      "train_size : 2688\n",
      "test_size : 475\n",
      "n_features : 156\n",
      "val_size (476, 26)\n"
     ]
    }
   ],
   "source": [
    "print('Dataset path :', dataset_path)\n",
    "print('n_stocks :', n_stocks)\n",
    "print('batch_size :', batch_size)\n",
    "\n",
    "# Import the dataset\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Import the dataset\n",
    "df_growths = df[[f'Growth.{i}' for i in range(1, n_stocks)] + ['Growth'] ]\n",
    "growths = df_growths.to_numpy()[1:]\n",
    "# Labels are same shape as growths, and for each day is 1 if it was the best growth, else 0\n",
    "labels = np.zeros_like(growths)\n",
    "for i in range(growths.shape[0]):\n",
    "    best_stock = np.argmax(growths[i])\n",
    "    labels[i][best_stock] = 1\n",
    "print('Growth size :', growths.shape)\n",
    "print('Labels :', labels, 'of size', labels.shape)\n",
    "\n",
    "# Normalizing data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_dataset = scaler.fit_transform(df[:-1])\n",
    "\n",
    "# Separating the train and test sets\n",
    "train_dataset, test_dataset = train_test_split(\n",
    "    scaled_dataset,\n",
    "    test_size=0.15,\n",
    "    shuffle=False\n",
    ")\n",
    "# Separating the train and test labels\n",
    "train_labels, test_labels = train_test_split(\n",
    "    labels,\n",
    "    test_size=0.15,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "train_dataloader = dataset_from_numpy(train_dataset, train_labels, context_size, batch_size)\n",
    "test_dataloader = dataset_from_numpy(test_dataset, test_labels, context_size, batch_size)\n",
    "df_val = df[[f'Close.{i}' for i in range(1, n_stocks)]+['Close']].to_numpy()\n",
    "\n",
    "val_indices = df_val[-test_dataset.shape[0]-1:]\n",
    "# Get training and test size\n",
    "train_size, input_dim = train_dataset.shape\n",
    "test_size, _ = test_dataset.shape\n",
    "\n",
    "print('train_size :', train_size)\n",
    "print('test_size :', test_size)\n",
    "print('n_features :', input_dim)\n",
    "print('val_size',val_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a01cfd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00074037 -0.01359539]\n",
      " [-0.02316505 -0.01432533]] [-0.02316505 -0.01432533]\n"
     ]
    }
   ],
   "source": [
    "print(df.to_numpy()[1:3,-2:],growths[1][-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95fd07c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): Encoder(\n",
      "    (blocks): ModuleList(\n",
      "      (0-4): 5 x EncoderBlock(\n",
      "        (multi_head_attention): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0-15): 16 x Head(\n",
      "              (Q): Linear(in_features=256, out_features=256, bias=False)\n",
      "              (K): Linear(in_features=256, out_features=256, bias=False)\n",
      "              (V): Linear(in_features=256, out_features=16, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): Dropout(p=0.05, inplace=False)\n",
      "          (2): GELU(approximate='none')\n",
      "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        )\n",
      "        (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (input_projection): Linear(in_features=156, out_features=256, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=26, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Number of Parameters : 13888538\n"
     ]
    }
   ],
   "source": [
    "# Importing a LstmNet\n",
    "model = Transformer.Transformer(input_dim,num_classes=n_stocks, embedding_size=context_size, dropout_rate=0.1, head_size=256, num_heads=16,n_encoder_blocks=5,max_context_size=context_size)\n",
    "# Adding a final layer for classification\n",
    "\n",
    "# Changing to appropriate device\n",
    "model.to(device)\n",
    "\n",
    "classif_only = True\n",
    "print(model)\n",
    "print(\"Number of Parameters :\", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50a9bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Training as Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d827edf6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'loa'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#Training.train_model(model, train_dataloader, test_dataloader, optimizer=optimizer_classif, criterion=criterion_classif , num_epochs=nepochs, device=device, batch_size=batch_size)\u001b[39;00m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m#loading model \u001b[39;00m\n\u001b[32m     10\u001b[39m path_model = \u001b[33m'\u001b[39m\u001b[33mtransformer_model.pth\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloa\u001b[49m\n\u001b[32m     14\u001b[39m torch.save(model.state_dict(), path_model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/__init__.py:2757\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m   2754\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _lazy_modules:\n\u001b[32m   2755\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[34m__name__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2757\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torch' has no attribute 'loa'"
     ]
    }
   ],
   "source": [
    "# Loss functions\n",
    "criterion_classif = torch.nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_classif = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#Training.train_model(model, train_dataloader, test_dataloader, optimizer=optimizer_classif, criterion=criterion_classif , num_epochs=nepochs, device=device, batch_size=batch_size)\n",
    "\n",
    "#loading model \n",
    "path_model = 'transformer_model.pth'\n",
    "\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), path_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2efc723a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the test set: 11.415525114155251 %\n"
     ]
    }
   ],
   "source": [
    "# accuracy calculation over test set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels_max = torch.max(labels.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels_max).sum().item()\n",
    "print('Test Accuracy of the model on the test set: {} %'.format(100 * correct / total)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4360412d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb428b81",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2944) must match the size of tensor b (256) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mautoreload\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m2\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtrading\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m trading_test_transformer\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mtrading_test_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_classifier\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalue_data\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/M1/Deep Learning/Transformer_deep_trading/trading.py:163\u001b[39m, in \u001b[36mtrading_test_transformer\u001b[39m\u001b[34m(data_init, data, model_classifier, context_size, value_init, value_data)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    162\u001b[39m     inp = inp.to(device)\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m output = \u001b[43mmodel_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# last timestep prediction\u001b[39;00m\n\u001b[32m    165\u001b[39m predicted_index = torch.argmax(output[\u001b[32m0\u001b[39m, -\u001b[32m1\u001b[39m]).item()  \u001b[38;5;66;03m# prédire le jour i+1\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/M1/Deep Learning/Transformer_deep_trading/Model_Blocks/Transformer.py:43\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x, mask_input)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask_input=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     42\u001b[39m     \u001b[38;5;66;03m# x: [batch, seq_len, input_dim] avec des valeurs continues\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     entry_embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_projection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpositional_encoding\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m     entry_embeddings = \u001b[38;5;28mself\u001b[39m.dropout(entry_embeddings)\n\u001b[32m     46\u001b[39m     \u001b[38;5;66;03m# Encoder traite toute la séquence\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (2944) must match the size of tensor b (256) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# autoreload module\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from trading import trading_test_transformer\n",
    "\n",
    "trading_test_transformer(train_dataset,test_dataset,model_classifier=model,context_size=context_size,value_data = val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49226fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
