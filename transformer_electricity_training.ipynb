{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a357784c",
   "metadata": {},
   "source": [
    "# Electricity predicteur\n",
    "Transformer based model for Electricity consommation prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5763e6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time, strftime, gmtime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from Model_Blocks.Transformer import Transformer\n",
    "from train_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28944431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "dataset_path = \"dataset.csv\"\n",
    "model_path = ''\n",
    "lr = 1e-3\n",
    "batch_size = 8\n",
    "save_dir = \"outputs\"\n",
    "nepochs = 10\n",
    "num_layers = 2\n",
    "hidden_dim = 64\n",
    "label_smoothing = 0.0\n",
    "\n",
    "# Transformer specific\n",
    "embedding_size = 64\n",
    "dropout_rate = 0.1\n",
    "head_size = 64\n",
    "num_heads = 4\n",
    "n_encoder_blocks = 2\n",
    "max_context_size = 100 \n",
    "\n",
    "col = 'AEP_MW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbe44ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing to mps if available\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "print('Device :', device)\n",
    "\n",
    "# Create the directory containing the model, the logs, etc.\n",
    "dir_name = strftime(\"%Y-%m-%d_%H-%M-%S\", gmtime())\n",
    "out_dir = os.path.join(save_dir, dir_name)\n",
    "os.makedirs(out_dir)\n",
    "\n",
    "path_model = os.path.join(out_dir, \"model.pth\")\n",
    "path_model_classif = os.path.join(out_dir, \"model_classif.pth\")\n",
    "path_config = os.path.join(out_dir, \"config.json\")\n",
    "path_logs = os.path.join(out_dir, \"logs.json\")\n",
    "\n",
    "# Save config\n",
    "with open(path_config, 'w') as f:\n",
    "    # storing variables is a bit hacky without argparse, just saving a dict manually or empty\n",
    "    # replicating user style who dumped vars(args)\n",
    "    config = {\n",
    "        \"dataset_path\": dataset_path,\n",
    "        \"lr\": lr, \n",
    "        \"batch_size\": batch_size,\n",
    "        \"nepochs\": nepochs,\n",
    "        \"hidden_dim\": hidden_dim\n",
    "    }\n",
    "    json.dump(config, f)\n",
    "    f.write('\\n')\n",
    "\n",
    "print('Dataset path :', dataset_path)\n",
    "print('batch_size :', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9dbc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "df = pd.read_csv(dataset_path)\n",
    "df = df[[col]]\n",
    "print(df)\n",
    "\n",
    "# Normalizing data\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_dataset = scaler.fit_transform(df)\n",
    "\n",
    "\n",
    "# Separating the train and test sets\n",
    "train_dataset, test_dataset = train_test_split(\n",
    "    scaled_dataset,\n",
    "    test_size=0.15,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Get training and test size\n",
    "train_size, input_dim = train_dataset.shape\n",
    "test_size, _ = test_dataset.shape\n",
    "\n",
    "print('train_size :', train_size)\n",
    "print('test_size :', test_size)\n",
    "print('n_features :', input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c27e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Training a model #\n",
    "####################\n",
    "\n",
    "\n",
    "# Importing a Transformer\n",
    "model = Transformer(\n",
    "    input_dim, \n",
    "    num_classes=input_dim,\n",
    "    embedding_size=embedding_size,\n",
    "    dropout_rate=dropout_rate, \n",
    "    head_size=head_size, \n",
    "    num_heads=num_heads,\n",
    "    n_encoder_blocks=n_encoder_blocks,\n",
    "    max_context_size=max_context_size\n",
    ")\n",
    "\n",
    "# Load weights if specified\n",
    "if model_path != '':\n",
    "    model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "\n",
    "# Changing to appropriate device\n",
    "model.to(device)\n",
    "\n",
    "print(model)\n",
    "print(\"Number of Parameters :\", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bda6071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "# Part 1 of training\n",
    "print('-- Training the model --')\n",
    "train_lstm(model,\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    nepochs,\n",
    "    path_logs = path_logs,\n",
    "    path_model = path_model,\n",
    "    batch_size = batch_size,\n",
    "    device = device)\n",
    "\n",
    "torch.save(model.state_dict(), path_model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
